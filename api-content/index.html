{"posts":[{"title":"ElasticSearch中composite聚合的使用【转】","content":"简介composite composite是一个多桶聚合，它从不同的源创建复合桶，与其他多桶聚合不同，复合聚合可用于高效地对多级聚合中的所有桶进行分页。这种聚合提供了一种方法来流特定聚合的所有桶，类似于滚动对文档所做的操作。 组合桶是由为每个文档提取/创建的值的组合构建的，每个组合被视为组合桶。如下为官方给的例子： { &quot;keyword&quot;: [&quot;foo&quot;, &quot;bar&quot;], &quot;number&quot;: [23, 65, 76] } 如果我们同时对keyword和number两个字段进行聚合会得出以下的结果: { &quot;keyword&quot;: &quot;foo&quot;, &quot;number&quot;: 23 } { &quot;keyword&quot;: &quot;foo&quot;, &quot;number&quot;: 65 } { &quot;keyword&quot;: &quot;foo&quot;, &quot;number&quot;: 76 } { &quot;keyword&quot;: &quot;bar&quot;, &quot;number&quot;: 23 } { &quot;keyword&quot;: &quot;bar&quot;, &quot;number&quot;: 65 } { &quot;keyword&quot;: &quot;bar&quot;, &quot;number&quot;: 76 } 看到上面的例子是不是恍然大悟， 就像类似sql中的多group by 多字段，可以对多个字段进行聚合，这非常适用于对于多维度出报表的需求，我这里建议使用的版本为6.5+，因为6.5版本以下此功能还处于测试阶段，设计和代码没有正式的GA功能成熟，并且没有担保。当然我这里也会提供6.5版本以下如何进行多聚合字段的使用。 首先上官方文档地址: https://www.elastic.co/guide/en/elasticsearch/reference/6.5/search-aggregations-bucket-composite-aggregation.html 其实官方文档已经把该功能说得很详细，如果你只是单纯写DSL实现的话看官方文档就可以，我接下来就介绍如何调用他的javaAPI来使用，当然如果阅读源码能力强的话也可以直接看官方在github的test,地址如下: https://github.com/elastic/elasticsearch/tree/master/server/src/test/java/org/elasticsearch/search/aggregations/bucket/composite 验证多字段聚合可行性 上面的例子为官方例子，我们需要自己弄个例子检查可行性，我们现在创建一个index,索引名为composite_test,mapping如下 { &quot;area&quot; : { &quot;type&quot; : &quot;keyword&quot; }, &quot;userid&quot; : { &quot;type&quot; : &quot;keyword&quot; }, &quot;sendtime&quot; : { &quot;type&quot; : &quot;date&quot;, &quot;format&quot; : &quot;yyyy-MM-dd HH:mm:ss&quot; } } 我们创建好了index，index中一共有三个字段，area，userid，sendtime三个字段，我们也使用MySQL数据建一个一模一样的表，方便对比聚合出来的数据是否正确，表名为composite_test。 我们首先对数据表composite_test插入5条记录，分别如下 使用group by对三个字段进行聚合，以下为在数据库中的实现： SELECT COUNT(1),area,userid,sendtime FROM composite_test GROUP BY area,userid,sendtime 结果如下: 以上为数据库的聚合实现，所以我们使用ES进行多字段聚合的时候如果结果和以上的一样则是正确的，我们也一样往ES composite_test索引中 插入相同的5条数据,在kibana上执行以下命令插入。 POST composite_test/_bulk { &quot;index&quot; : {&quot;_type&quot; :&quot;_doc&quot;}} {&quot;area&quot;:&quot;33&quot;,&quot;userid&quot;:&quot;400015&quot;,&quot;sendtime&quot;:&quot;2019-01-17 00:00:00&quot;} { &quot;index&quot; : {&quot;_type&quot; : &quot;_doc&quot;}} {&quot;area&quot;:&quot;33&quot;,&quot;userid&quot;:&quot;400015&quot;,&quot;sendtime&quot;:&quot;2019-01-17 00:00:00&quot;} { &quot;index&quot; : {&quot;_type&quot; : &quot;_doc&quot;}} {&quot;area&quot;:&quot;35&quot;,&quot;userid&quot;:&quot;400016&quot;,&quot;sendtime&quot;:&quot;2019-01-18 00:00:00&quot;} { &quot;index&quot; : { &quot;_type&quot; : &quot;_doc&quot;}} {&quot;area&quot;:&quot;35&quot;,&quot;userid&quot;:&quot;400016&quot;,&quot;sendtime&quot;:&quot;2019-01-18 00:00:00&quot;} { &quot;index&quot; : {&quot;_type&quot; : &quot;_doc&quot;}} {&quot;area&quot;:&quot;33&quot;,&quot;userid&quot;:&quot;400017&quot;,&quot;sendtime&quot;:&quot;2019-01-17 00:00:00&quot;} 查询ES，我们发现已经存在了这5条数据了。 接下来我们先使用composite的DSL查询： GET composite_test/_search { &quot;size&quot;: 0, &quot;aggs&quot; : { &quot;my_buckets&quot;: { &quot;composite&quot; : { &quot;sources&quot; : [ { &quot;area&quot;: { &quot;terms&quot;: {&quot;field&quot;: &quot;area&quot; } } }, { &quot;userid&quot;: { &quot;terms&quot;: {&quot;field&quot;: &quot;userid&quot; } } }, { &quot;sendtime&quot;: { &quot;date_histogram&quot;: { &quot;field&quot;: &quot;sendtime&quot;,&quot;interval&quot;: &quot;1d&quot;,&quot;format&quot;: &quot;yyyy-MM-dd&quot;} } } ] } } } } DSL中参数的意思请自行参考官方文档，都说得很详细的,以上这句理论上是等值于数据库的那句聚合语句的，查询结果如下 [ { &quot;key&quot; : { &quot;area&quot; : &quot;33&quot;, &quot;userid&quot; : &quot;400015&quot;, &quot;sendtime&quot; : &quot;2019-01-17&quot; }, &quot;doc_count&quot; : 2 }, { &quot;key&quot; : { &quot;area&quot; : &quot;33&quot;, &quot;userid&quot; : &quot;400017&quot;, &quot;sendtime&quot; : &quot;2019-01-17&quot; }, &quot;doc_count&quot; : 1 }, { &quot;key&quot; : { &quot;area&quot; : &quot;35&quot;, &quot;userid&quot; : &quot;400016&quot;, &quot;sendtime&quot; : &quot;2019-01-18&quot; }, &quot;doc_count&quot; : 2 } ] 从以上响应的json数组中我们不难看出，该聚合聚合出来的数据是和数据库聚合出来的数据是一致的，所以 composite是可以使用在多字段聚合上的。论证完可行性，我们接下来使用java来实现，实话说这一块我是看源代码才会使用的，网上资料基本为0，而且官方java使用文档里也没用，的确是与遇到了不少坑，写出来方便以后使用能快速回忆。 java使用composite聚合 首先创建Maven项目,加入ElasticSearch依赖： &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;6.5.4&lt;/version&gt; &lt;/dependency&gt; 直接上实现代码: SearchRequest searchRequest = new SearchRequest(&quot;composite_test&quot;); searchRequest.types(&quot;_doc&quot;); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.size(0); /********************以下组装聚合的三个字段****************************/ List&lt;CompositeValuesSourceBuilder&lt;?&gt;&gt; sources = new ArrayList&lt;&gt;(); DateHistogramValuesSourceBuilder sendtime = new DateHistogramValuesSourceBuilder(&quot;sendtime&quot;) .field(&quot;sendtime&quot;) .dateHistogramInterval(DateHistogramInterval.days(1)) .format(&quot;yyyy-MM-dd&quot;).order(SortOrder.DESC).missingBucket(false); sources.add(sendtime); TermsValuesSourceBuilder userid = new TermsValuesSourceBuilder(&quot;userid&quot;).field(&quot;userid&quot;).missingBucket(true); sources.add(userid); TermsValuesSourceBuilder dttype = new TermsValuesSourceBuilder(&quot;area&quot;).field(&quot;area&quot;).missingBucket(true); sources.add(dttype); CompositeAggregationBuilder composite =new CompositeAggregationBuilder(&quot;my_buckets&quot;, sources); composite.size(1000); /*********************执行查询******************************/ searchSourceBuilder.aggregation(composite); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = client.search(searchRequest,RequestOptions.DEFAULT); /********************取出数据*******************/ Aggregations aggregations = searchResponse.getAggregations(); ParsedComposite parsedComposite = aggregations.get(&quot;my_buckets&quot;); List&lt;ParsedBucket&gt; list = parsedComposite.getBuckets(); Map&lt;String,Object&gt; data = new HashMap&lt;&gt;(); for(ParsedBucket parsedBucket:list){ data.clear(); for (Map.Entry&lt;String, Object&gt; m : parsedBucket.getKey().entrySet()) { data.put(m.getKey(),m.getValue()); } data.put(&quot;count&quot;,parsedBucket.getDocCount()); System.out.println(data); } /*************************************/ 代码中的client为RestHighLevelClient，请自行初始化，然后执行代码，控制台打印如下: {area=35, count=2, sendtime=2019-01-18, userid=400016} {area=33, count=2, sendtime=2019-01-17, userid=400015} {area=33, count=1, sendtime=2019-01-17, userid=400017} 数据正确，方法可用，其实这个方法是RestHighLevelClient替我们封装了composite生成DSL。 这里注意一下missingBucket的设置，这个的意思是如果该字段没值，为true的时候会返回null，为false不返回整条数据，注意这里是整条数据，而不是单单这个字段而已。 低版本使用ES多字段聚合 以上就是composite的验证和在java中的使用方法，建议在6.5+版本使用，但这个时候小伙伴可能会问，如果我是6.5以下的版本呢，这里我也有一个方法，只不过是有点绕，直接上代码： SearchRequest searchRequest = new SearchRequest(&quot;composite_test&quot;); searchRequest.types(&quot;_doc&quot;); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.size(0); /********************以下组装聚合的三个字段****************************/ AggregationBuilder sendtime=AggregationBuilders.dateHistogram(&quot;sendtime&quot;).field(&quot;sendtime&quot;).format(&quot;yyyy-MM-dd&quot;).interval(86400000); AggregationBuilder area=AggregationBuilders.terms(&quot;area&quot;).field(&quot;area&quot;); AggregationBuilder userid=AggregationBuilders.terms(&quot;userid&quot;).field(&quot;userid&quot;); //实现功能关键点 area.subAggregation(userid); sendtime.subAggregation(area); /*********************执行查询******************************/ searchSourceBuilder.aggregation(sendtime); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = client.search(searchRequest,RequestOptions.DEFAULT); /********************取出数据*******************/ Aggregations aggregations = searchResponse.getAggregations(); //取出数据 aggHandle(aggregations); /*************************************/ 运行响应的数据为 {area=33, count=2, sendtime=2019-01-17, userid=400015}, {area=33, count=1, sendtime=2019-01-17, userid=400017}, {area=35, count=2, sendtime=2019-01-18, userid=400016} 所以使用这个方法也能多值聚合数据，其中实现的关键点在于 area.subAggregation(userid); sendtime.subAggregation(area); 这里相当于把每个桶给串联起来，串联顺序无要求，但用这种方法最后取数据的时候比较麻烦，我这里也分享一个我取数据的方法 private static List&lt;Map&lt;String,Object&gt;&gt; listmap =new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); private static Map&lt;String,Object&gt; map =new HashMap&lt;String,Object&gt;(16); //使用递归的方式将聚合数据中的数据一一取出来 private static void aggHandle(Aggregations agg){ String name =&quot;&quot;; Long longValue = 0L; for(Aggregation data:agg){ name = data.getName(); Object obj = agg.get(name); if(obj instanceof Terms){ Terms terms = (Terms) obj; name = terms.getName(); if(terms.getBuckets().size()==0){ listmap.add(clonMap(map)); } for (Terms.Bucket entry : terms.getBuckets()) { map.put(name, entry.getKey()); longValue = entry.getDocCount(); map.put(count, longValue); List&lt;Aggregation&gt; list = entry.getAggregations().asList(); if(list==null||list.isEmpty()){ listmap.add(clonMap(map)); }else{ aggHandle(entry.getAggregations()); } } }else if(obj instanceof ParsedDateHistogram){ ParsedDateHistogram terms = (ParsedDateHistogram) obj; List&lt;? extends Bucket&gt; buckets = terms.getBuckets(); name = terms.getName(); if(buckets.size()==0){ listmap.add(clonMap(map)); } for(Bucket entry:buckets){ map.put(name,entry.getKeyAsString()); longValue = entry.getDocCount(); map.put(count, longValue); List&lt;Aggregation&gt; list = entry.getAggregations().asList(); if(list==null||list.isEmpty()){ listmap.add(clonMap(map)); //map.clear(); }else{ aggHandle(entry.getAggregations()); } } }else if(obj instanceof Max){ Max max =(Max) obj; name = max.getName(); Double value = max.getValue(); longValue =value.longValue(); map.put(name, longValue); listmap.add(clonMap(map)); }else if(obj instanceof Min){ Min min =(Min) obj; Double value = min.getValue(); longValue =value.longValue(); map.put(min.getName(), longValue); listmap.add(clonMap(map)); }else if(obj instanceof Avg){ Avg avg = (Avg) obj; Double value = avg.getValue(); longValue =value.longValue(); map.put(avg.getName(), longValue); listmap.add(clonMap(map)); }else if(obj instanceof Sum){ Sum sum = (Sum) obj; Double value = sum.getValue(); longValue =value.longValue(); map.put(sum.getName(), longValue); listmap.add(clonMap(map)); }else if(obj instanceof ValueCount){ ValueCount count = (ValueCount) obj; longValue =count.getValue(); map.put(count.getName(), longValue); listmap.add(clonMap(map)); }else{ System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;+obj); } } } /** * 克隆map对象到另外一个map对象里面去 * @param map * @return */ private static Map&lt;String,Object&gt; clonMap(Map&lt;String,Object&gt; mapTo){ Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;(16); for (Map.Entry&lt;String,Object&gt; entry : mapTo.entrySet()) { String key = entry.getKey(); Object value = entry.getValue(); map.put(key, value); } return map; } https://blog.csdn.net/qq_18895659/article/details/86540548 ","link":"https://kychill1986.github.io/post/elasticsearch-zhong-composite-ju-he-de-shi-yong-zhuan/"},{"title":"Java 8 BiFunction 和 BiConsumer使用","content":"BiFunction 这里提醒一点，可以看出该接口中有方法的实现，不单单只有抽象方法，在java8中，有一个新的改进就是在现有的接口中增加了一些默认的方法实现，使用default关键字来修饰。这种做法不会影响以前代码对接口的实现，也对原有的接口进行了扩展。 传递行为给方法，而不是传递值。 /** * BiFunction是一个函数式接口，它接口两个参数并产生一个结果值返回。 * 它里面有一个apply(Object,Object)方法。 * @param &lt;T&gt; 函数的第一个参数类型 * @param &lt;U&gt; 函数的第二个参数类型 * @param &lt;R&gt; 函数的结果类型 * @since 1.8 */ @FunctionalInterface public interface BiFunction&lt;T, U, R&gt; { /** * 将apply函数应用到给定的参数上面 * * @param t 函数的第一个参数 * @param u 函数的第二个参数 * @return R 函数的结果 */ R apply(T t, U u); /** * 返回一个组合的函数，第一次是将该函数应用到它的输入，接着是将该函数的after应用到 * 之前的结果上。如果在任一函数评测期间抛出异常，它都会被传递给组合函数的调用者。 * @param &lt;V&gt; 组合函数和after函数的输出类型 * @param after 该函数应用将被在当前函数apply后被apply * @return 返回一个组合函数，第一次应用该函数，接着应用after函数 * @throws 当after为null的时候，会抛出NullPointerException异常。 */ default &lt;V&gt; BiFunction&lt;T, U, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) { Objects.requireNonNull(after); return (T t, U u) -&gt; after.apply(apply(t, u)); } } import java.util.function.BiFunction; import java.util.function.Function; public class BiFunctionTest { public static void main(String[] args){ BiFunctionTest test = new BiFunctionTest(); //实现四则运算 System.out.println(test.compute(4,2,(value1,value2)-&gt;value1+value2)); System.out.println(test.compute(4,2,(v1,v2)-&gt;v1-v2)); System.out.println(test.compute(1,2,(v1,v2)-&gt;v1*v2)); System.out.println(test.compute(3,2,(v1,v2)-&gt;v1/v2)); System.out.println(test.calcute(3,4,(v1,v2)-&gt;v1+v2,v-&gt;v * v)); } public int compute(int num1, int num2, BiFunction&lt;Integer,Integer,Integer&gt; biFunction){ return biFunction.apply(num1,num2); } public int calcute(int num1, int num2, BiFunction&lt;Integer,Integer,Integer&gt; biFunction, Function&lt;Integer,Integer&gt; function){ //调用addThen首先对接收的两个参数进行bifunction的apply，然后在进行function的apply return biFunction.andThen(function).apply(num1,num2); } } 输出结果： 6 2 2 1 49 BiConsumer BiConsumer&lt;T,U&gt;接口是java8 Function函数里面的接口，它有两个方法， BiConsumer用法很简单。 //接收参数 void accept(T t, U u) //默认方法，基本没什么用 default BiConsumer&lt;T,U&gt; andThen(BiConsumer&lt;? super T,? super U&gt; after) BiConsumer用法示例(java8)示例如下，andThen(biConsumer)方法没有什么效果(可不用)，主要是学会使用accept(T t, U u)方法，如下。 import java.util.function.BiConsumer; public class TestDemo { public static void main(String[] args) { BiConsumer&lt;String, String&gt; biConsumer = (x, y) -&gt; { System.out.println(x+&quot;===&quot;+y); }; biConsumer.andThen(biConsumer).accept(&quot;tpyyes.com &quot;, &quot; java8&quot;); } } 输出x与y参数的值，如下。 tpyyes.com === java8 tpyyes.com === java8 https://www.tpyyes.com/a/java/283.html http://www.manongjc.com/article/50415.html ","link":"https://kychill1986.github.io/post/java-8-bifunction-he-biconsumer-shi-yong/"},{"title":"Elasticsearch 聚合分页排序","content":"分页 请求JSON： GET oms_sku_shortage_order_index/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;oos&quot;: true } } ] } }, &quot;aggs&quot;: { &quot;by_wog&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;wog.keyword&quot;, &quot;size&quot;: 100000000 }, &quot;aggs&quot;: { &quot;sum_num&quot;: { &quot;sum&quot;: { &quot;field&quot;: &quot;num&quot; } }, &quot;bucket_field&quot;: { &quot;bucket_sort&quot;: { &quot;sort&quot;: [ { &quot;sum_num&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;from&quot;: 0, &quot;size&quot;: 10 } } } } } } Java代码： SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolBuilder = QueryBuilders.boolQuery(); boolBuilder.must(QueryBuilders.matchQuery(&quot;oos&quot;, true)); if (CollectionUtils.isNotEmpty(pageQueryShortageBO.getWarehouseCodeList())) { boolBuilder.must(QueryBuilders.termsQuery(&quot;w.keyword&quot;, w)); } if (CollectionUtils.isNotEmpty(pageQueryShortageBO.getOwnerCodeList())) { boolBuilder.must(QueryBuilders.termsQuery(&quot;o.keyword&quot;, o)); } if (CollectionUtils.isNotEmpty(pageQueryShortageBO.getGoodsCodeList())) { boolBuilder.must(QueryBuilders.termsQuery(&quot;g.keyword&quot;, g)); } sourceBuilder.query(boolBuilder); TermsAggregationBuilder aggregation = AggregationBuilders.terms(&quot;by_wog&quot;).field(&quot;wog.keyword&quot;).size(100000); aggregation.subAggregation(AggregationBuilders.sum(&quot;sum_num&quot;).field(&quot;num&quot;)); List&lt;FieldSortBuilder&gt; fieldSorts = new ArrayList&lt;&gt;(); fieldSorts.add(new FieldSortBuilder(&quot;sum_num&quot;)); aggregation.subAggregation(new BucketSortPipelineAggregationBuilder(&quot;bucket_field&quot;, fieldSorts).from((pageQueryShortageBO.getPageNum() - 1) * pageQueryShortageBO.getPageSize()).size(pageQueryShortageBO.getPageSize())); sourceBuilder.aggregation(aggregation); SearchRequest searchRequest = new SearchRequest(&quot;shortage_index&quot;); searchRequest.types(ESConstants.TYPE); searchRequest.source(sourceBuilder); SearchResponse response = null; try { response = client.search(searchRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(&quot;queryShortageFromES error: {}&quot;, e.getMessage(), e); } if (response != null) { Aggregations aggregations = response.getAggregations(); Terms byCompanyAggregation = aggregations.get(&quot;by_wog&quot;); List&lt;? extends Terms.Bucket&gt; buckets = byCompanyAggregation.getBuckets(); for (Terms.Bucket bucket : buckets) { Sum sum = bucket.getAggregations().get(&quot;sum_num&quot;); String wog = bucket.getKeyAsString(); String[] wogArray = wog.split(ESConstants.WOG_SPLIT_FLAG); } } 说明：因为bucket_sort是对聚合之后的桶进行分页排序,所以terms里面的size设置要大于bucket_sort里面分页大小，不然会取不到数据 计算count 请求JSON： GET oms_sku_shortage_order_index/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ {&quot;match&quot;: { &quot;oos&quot;: true }} ] } }, &quot;aggs&quot;: { &quot;by_wog&quot;: { &quot;cardinality&quot;: { &quot;field&quot;: &quot;wog.keyword&quot; } } } } Java代码： SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolBuilder = QueryBuilders.boolQuery(); boolBuilder.must(QueryBuilders.matchQuery(&quot;oos&quot;, true)); if (CollectionUtils.isNotEmpty(pageQueryShortageBO.getWarehouseCodeList())) { boolBuilder.must(QueryBuilders.termsQuery(&quot;w.keyword&quot;, w)); } if (CollectionUtils.isNotEmpty(pageQueryShortageBO.getOwnerCodeList())) { boolBuilder.must(QueryBuilders.termsQuery(&quot;o.keyword&quot;, o)); } if (CollectionUtils.isNotEmpty(pageQueryShortageBO.getGoodsCodeList())) { boolBuilder.must(QueryBuilders.termsQuery(&quot;g.keyword&quot;, g)); } sourceBuilder.query(boolBuilder); CardinalityAggregationBuilder aggregation = AggregationBuilders.cardinality(&quot;by_wog&quot;).field(&quot;wog.keyword&quot;); sourceBuilder.aggregation(aggregation); SearchRequest searchRequest = new SearchRequest(&quot;shortage_index&quot;); searchRequest.types(ESConstants.TYPE); searchRequest.source(sourceBuilder); SearchResponse response = null; try { response = client.search(searchRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(&quot;getShortageTotalPage error: {}&quot;, e.getMessage(), e); } if (response != null) { Aggregations aggregations = response.getAggregations(); Cardinality byCompanyAggregation = aggregations.get(&quot;by_wog&quot;); return Integer.parseInt(String.valueOf(byCompanyAggregation.getValue())); } ","link":"https://kychill1986.github.io/post/elasticsearch-ju-he-fen-ye-pai-xu/"},{"title":"虚拟机内的ElasticSearch 7.6.0，可以在宿主机用IP进行访问(CentOS)","content":"🖥 ElasticSearch 对外可访问配置 1.关闭防火墙，或者加例外 a. 查看firewall服务状态 systemctl status firewalld b. 查看firewall的状态 firewall-cmd --state d. 开启、重启、关闭、firewalld.service服务 # 开启 service firewalld start # 重启 service firewalld restart # 关闭 service firewalld stop e. 查看防火墙规则 firewall-cmd --list-all f. 查询、开放、关闭端口 # 查询端口是否开放 firewall-cmd --query-port=8080/tcp # 开放80端口 firewall-cmd --permanent --add-port=80/tcp # 移除端口 firewall-cmd --permanent --remove-port=8080/tcp #重启防火墙(修改配置后要重启防火墙) firewall-cmd --reload # 参数解释 1、firwall-cmd：是Linux提供的操作firewall的一个工具； 2、--permanent：表示设置为持久； 3、--add-port：标识添加的端口； 2. 修改elasticsearch.yml network.host: 192.168.33.134 #可以直接写部署ES的IP http.port: 9200 node.name: node-1 bootstrap.memory_lock: false bootstrap.system_call_filter: false cluster.initial_master_nodes: [&quot;node-1&quot;] #这个“node-1”其实就是上面的“node.name”配置项 3. 修改linux系统配置 修改sysctl.conf sudo vim /etc/sysctl.conf vm.max_map_count=262144 检查配置是否生效 sysctl -a | grep “vm.max_map_count” 显示vm.max_map_count = 262144 编辑/etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 * soft nproc 4096 * hard nproc 4096 #以上的配置是针对root用户的 chill soft nofile 65536 chill hard nofile 65536 #以上的配置是针对chill用户的 这样在宿主机输入http://192.168.89.128:9200/，就可以看到es正常启动了 启动时查看elasticsearch.log日志，如果会看到如下的错误提示，那么按照上面的elasticsearch.yml，就可以解决，如果解决不了，可以针对问题直接baidu [3] bootstrap checks failed [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535] [2]: max number of threads [3790] for user [chill] is too low, increase to at least [4096] [3]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, clus ter.initial_master_nodes] must be configured ","link":"https://kychill1986.github.io/post/xu-ni-ji-nei-de-elasticsearch-760ke-yi-zai-su-zhu-ji-yong-ip-jin-xing-fang-wen-centos/"},{"title":"Mac 系统安装ElasticSearch","content":"🖥 包含单机安装，以及集群配置介绍... 单机安装 下载elasticSearch http://elastic.co 解压elasticSearch...tar.gz 启动elasticSearch ./bin/elasticsearch -d #后台运行 安装elasticSearch-head插件 下载elasticSearch-head，访问gitHub，搜索elasticSearch-head，在搜索结果里点击“mobz/elasticsearch-head” 解压elasticSearch-head cd '/Users/xxx/Documents/Tools/ElasticSearch/elasticsearch-head-master/' 安装最新版node brew install node 安装grunt npm install -g grunt-cli 在elasticsearch-head-master目录执行 npm install 如果安装过程中出现： 1. Failed at the phantomjs-prebuilt@2.1.15 install script ‘node install.js’. 解决方法： npm install phantomjs-prebuilt@2.1.15 --ignore-scripts 2. Local Npm module &quot;grunt-contrib-jasmine&quot; not found. Is it installed 解决办法： npm install grunt-contrib-jasmine --registry=https://registry.npm.taobao.org #使用国内镜像 启动elasticSearch-head npm run start 测试：http://serverip:9100/ 配置elasticsearch 使elasticSearch-head能正常访问（这两个是独立的进程，有跨域问题），修改elasticsearch，config/elasticsearch.yml文件，加入如下代码： http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 重新启动elasticsearch，访问elasticSearch-head，http://serverip:9100/ ElasticSearch集群配置 修改config/elasticsearch.yml文件 master: http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; cluster.name: chillcluster node.name: chillmaster node.master: true slave1: cluster.name: chillcluster node.name: slave1 network.host: 127.0.0.1 http.port: 9201 discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] #找到master，如果不做这个配置那么这个节点就是游离于集群之外的 slave2: cluster.name: chillcluster node.name: slave2 network.host: 127.0.0.1 http.port: 9202 discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] #找到master，如果不做这个配置那么这个节点就是游离于集群之外的 #彩蛋🎆 官方下载下来的elasticsearch安装包，直接解压，运行以前命令，也可以实现同样的集群效果 复杂的： ./elasticsearch -E node.name=chillmaster -E cluster.name=chillcluster -E node.master=true -E http.cors.enabled=true -E http.cors.allow-origin=&quot;*&quot; -E path.data=node1_data -d ./elasticsearch -E node.name=slave1 -E cluster.name=chillcluster -E network.host=127.0.0.1 -E http.port=9201 -E discovery.zen.ping.unicast.hosts=&quot;127.0.0.1&quot; -E path.data=node2_data -d ./elasticsearch -E node.name=slave2 -E cluster.name=chillcluster -E network.host=127.0.0.1 -E http.port=9202 -E discovery.zen.ping.unicast.hosts=&quot;127.0.0.1&quot; -E path.data=node3_data -d 简单的： ./elasticsearch -E node.name=node1 -E cluster.name=chillcluster -E path.data=node1_data -d ./elasticsearch -E node.name=node2 -E cluster.name=chillcluster -E path.data=node2_data -d ./elasticsearch -E node.name=node3 -E cluster.name=chillcluster -E path.data=node3_data -d 最后用http://127.0.0.1:9200/_cat/nodes，检查下是否正常 Reference https://blog.csdn.net/fenglailea/article/details/52934263 http://www.bubuko.com/infodetail-2108888.html ","link":"https://kychill1986.github.io/post/mac-xi-tong-an-zhuang-elasticsearch/"}]}